#cloud-config
users:
  - name: slurm
    groups: sudo
    shell: /bin/bash
    sudo: 'ALL=(ALL) NOPASSWD:ALL'
    ssh-authorized-keys:
      - ${chomp(ssh_public_key)}
      - ${master_pubkey}

packages:
  - munge
  - slurmd
  - slurm-client
  - ca-certificates
  - curl
  - gnupg
  - jq
  - numactl
  - make
  - gcc
  - libelf-dev
  - libssl-dev
  - bc
  - flex
  - bison
  - squashfs-tools
  - parallel
  - libnvidia-container-tools
  - pigz
  - squashfuse
  - fuse-overlayfs
  - bsdmainutils
  - ncal
  - libslurm-dev
  - nfs-common
  - glusterfs-client
  - openmpi-bin
  - openmpi-common
  - libopenmpi-dev

apt:
  conf: |
    Acquire::Retries "100";
    DPkg::Lock::Timeout "300";

runcmd:
  - systemctl disable unattended-upgrades.service
  - systemctl disable apt-daily.timer
  - systemctl disable apt-daily-upgrade.timer
  - sudo systemctl stop unattended-upgrades
  - wget -O - https://monitoring.api.nemax.nebius.cloud/monitoring/v2/gpu-metrics-exporter/install.sh | bash;
  - while fuser /var/lib/dpkg/lock >/dev/null 2>&1; do sleep 1; done; apt-get install -o Dpkg::Lock::Timeout=300 -o Acquire::Retries=100 -y munge slurmd slurm-client ca-certificates curl gnupg jq numactl make gcc libelf-dev libssl-dev bc flex bison squashfs-tools parallel libnvidia-container-tools pigz squashfuse fuse-overlayfs bsdmainutils ncal libslurm-dev nfs-common glusterfs-client openmpi-bin openmpi-common libopenmpi-dev
  - export DEBIAN_FRONTEND=noninteractive
  - wget https://github.com/NVIDIA/enroot/releases/download/v${ENROOT_VERSION}/enroot_${ENROOT_VERSION}-1_amd64.deb
  - wget https://github.com/NVIDIA/enroot/releases/download/v${ENROOT_VERSION}/enroot+caps_${ENROOT_VERSION}-1_amd64.deb
  - while fuser /var/lib/dpkg/lock >/dev/null 2>&1; do sleep 1; done; dpkg -i enroot_${ENROOT_VERSION}-1_amd64.deb
  - while fuser /var/lib/dpkg/lock >/dev/null 2>&1; do sleep 1; done; dpkg -i enroot+caps_${ENROOT_VERSION}-1_amd64.deb
  - git clone https://github.com/NVIDIA/pyxis.git && cd pyxis
  - CPPFLAGS='-I /usr/include/slurm' make && CPPFLAGS='-I /usr/include/slurm' make install
  - mkdir -p /var/lib/slurm/pyxis
  - echo "required /usr/local/lib/slurm/spank_pyxis.so runtime_path=/var/lib/slurm/pyxis" | tee /etc/slurm/plugstack.conf.d/pyxis.conf
  - mkdir -p /var/lib/slurm/enroot/ /var/spool/slurmd  /var/spool/enroot && chown -R slurm:slurm /var/lib/slurm/ /var/spool/slurmd/  /var/spool/enroot
  - mkdir -p /etc/systemd/system/slurmd.service.d/
  - mkdir -p /run/slurm
  - chown slurm:slurm /etc/slurm/slurmdbd.conf
  - chown slurm:slurm /var/spool/slurmd
  - chown slurm:slurm /run/slurm
  - echo 'export NCCL_TOPO_FILE=/etc/nccl-topo-h100-v1.xml' >> /home/slurm/.bashrc
  - chmod a+rwx /var/spool/enroot
  %{ if shared_fs_type == "filestore" }
  - mkdir /mnt/slurm
  - echo "slurm-fs /mnt/slurm virtiofs rw 0 0" >> /etc/fstab
  - mount -a
  %{ endif }
  %{ if shared_fs_type == "nfs" }
  - mkdir /mnt/slurm
  - echo "${nfs_ip}:/nfs /mnt/slurm nfs defaults 0 0" >> /etc/fstab
  - mount -a
  %{ endif }
  %{ if shared_fs_type == "gluster" }
  - mkdir /mnt/slurm
  - echo "gluster01:/stripe-volume /mnt/slurm glusterfs defaults,_netdev 0 0" >> /etc/fstab
  - mount -a
  %{ endif }

write_files:
- path: /etc/systemd/system/slurmd.service.d/local.conf
  content: |
    [Service]
    User=root
    Group=root
    PIDFile=/run/slurm/slurmd.pid
- path: /etc/enroot/enroot.conf.d/enroot.conf
  content: |
    ENROOT_RUNTIME_PATH        /var/spool/enroot/user-$(id -u)
    ENROOT_CONFIG_PATH         $${HOME}/enroot
    ENROOT_CACHE_PATH          /var/spool/enroot
    ENROOT_DATA_PATH           /var/spool/enroot/data/user-$(id -u)
    ENROOT_SQUASH_OPTIONS -noI -noD -noF -noX -no-duplicates
    ENROOT_ROOTFS_WRITABLE     yes
    ENROOT_MOUNT_HOME          no
    ENROOT_RESTRICT_DEV        no
- path: /etc/slurm/slurm.conf
  content: |
    # slurm.conf file generated by configurator.html.
    # Put this file on all nodes of your cluster.
    # See the slurm.conf man page for more information.
    #
    ClusterName=Slurm-cluster
    SlurmctldHost=${node_prefix}-master

    SlurmUser=slurm
    SlurmdUser=root
    SlurmctldPort=6817
    SlurmdPort=6818
    AuthType=auth/munge
    StateSaveLocation=/var/lib/slurm/slurmctld
    SwitchType=switch/none
    MpiDefault=pmi2
    SlurmctldPidFile=/run/slurmctld.pid
    SlurmdPidFile=/run/slurm/slurmd.pid
    ProctrackType=proctrack/pgid
    ReturnToService=0

    # TIMERS
    SlurmctldTimeout=300
    SlurmdTimeout=300
    InactiveLimit=0
    MinJobAge=300
    KillWait=30
    Waittime=0

    # DEBUG
    DebugFlags=NO_CONF_HASH

    # LOGGING/ACCOUNTNG
    SlurmctldDebug=info
    SlurmctldLogFile=/var/log/slurm/slurmctld.log
    SlurmdDebug=info
    SlurmdLogFile=/var/log/slurm/slurmd.log
    JobAcctGatherType=jobacct_gather/none

    #DB
    %{ if is_mysql }
    AccountingStorageType=accounting_storage/slurmdbd
    AccountingStorageHost=node-master
    JobCompType=jobcomp/mysql
    JobCompUser=slurm
    JobCompPass= ${password}
    JobCompHost= ${hostname}
    JobCompLoc=slurm-db
    %{ endif }

    GresTypes=gpu
    SelectType=select/cons_tres
    # COMPUTE NODES
    # NodeName=${node_prefix}-[1-${cluster_nodes_count}] CPUs=16 RealMemory=32090 State=idle State=UNKNOWN
    NodeName=${node_prefix}-[1-${cluster_nodes_count}] Gres=gpu:8 CPUs=160 RealMemory=1290080 State=idle State=UNKNOWN
    PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP
- path: /etc/slurm/cgroup.conf
  content: |
    CgroupReleaseAgentDir="/etc/slurm/cgroup"
- path: /etc/slurm/slurmdbd.conf
  permissions: '0600'
  content: |
    AuthType=auth/munge
    DbdHost=${node_prefix}-master
    DebugLevel=info
    LogFile=/var/log/slurm/slurmdbd.log
    PidFile=/run/slurmdbd.pid
    SlurmUser=slurm
    StoragePass=${password}
    StorageUser=slurm
    StorageHost=${hostname}
    StorageLoc=slurm-db
    StorageType=accounting_storage/mysql
- path: /etc/slurm/gres.conf
  content: |
    ${indent(4, file("files/gres.conf"))}
- path: /etc/tmpfiles.d/slurm.conf
  content: |
    d /run/slurm 0770 root slurm -
- path: /etc/nccl-topo-h100-v1.xml
  content: |
    ${indent(4, file("files/nccl-topo-h100-v1.xml"))}
