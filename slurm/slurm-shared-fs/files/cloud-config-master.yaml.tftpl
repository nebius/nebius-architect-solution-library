#cloud-config
users:
  - name: slurm
    groups: sudo
    shell: /bin/bash
    sudo: 'ALL=(ALL) NOPASSWD:ALL'
    ssh-authorized-keys:
      - ${chomp(ssh_public_key)}
      - ${master_pubkey}

packages:
  - munge
  - slurmdbd
  - slurm-client
  - slurmctld
  - ca-certificates
  - curl
  - gnupg
  - jq
  - numactl
  - make
  - gcc
  - libelf-dev
  - libssl-dev
  - bc
  - flex
  - bison
  - squashfs-tools
  - parallel
  - pigz
  - squashfuse
  - fuse-overlayfs
  - bsdmainutils
  - ncal
  - libslurm-dev
  - nfs-common
  - glusterfs-client

apt:
  conf: |
    Acquire::Retries "100";
    DPkg::Lock::Timeout "300";

runcmd:
  - systemctl disable unattended-upgrades.service
  - systemctl disable apt-daily.timer
  - systemctl disable apt-daily-upgrade.timer
  - sudo systemctl stop unattended-upgrades
  - while fuser /var/lib/dpkg/lock >/dev/null 2>&1; do sleep 1; done; apt-get install -o Dpkg::Lock::Timeout=300 -o Acquire::Retries=100 -y munge slurmdbd slurm-client slurmctld ca-certificates curl gnupg jq numactl make gcc libelf-dev libssl-dev bc flex bison squashfs-tools parallel pigz squashfuse fuse-overlayfs bsdmainutils ncal libslurm-dev nfs-common glusterfs-client
  - systemctl disable slurmd; systemctl enable slurmctld;
  - export DEBIAN_FRONTEND=noninteractive
  - wget https://github.com/NVIDIA/enroot/releases/download/v${ENROOT_VERSION}/enroot_${ENROOT_VERSION}-1_amd64.deb
  - wget https://github.com/NVIDIA/enroot/releases/download/v${ENROOT_VERSION}/enroot+caps_${ENROOT_VERSION}-1_amd64.deb
  - while fuser /var/lib/dpkg/lock >/dev/null 2>&1; do sleep 1; done; dpkg -i enroot_${ENROOT_VERSION}-1_amd64.deb
  - while fuser /var/lib/dpkg/lock >/dev/null 2>&1; do sleep 1; done; dpkg -i enroot+caps_${ENROOT_VERSION}-1_amd64.deb
  - git clone https://github.com/NVIDIA/pyxis.git && cd pyxis
  - CPPFLAGS='-I /usr/include/slurm' make && CPPFLAGS='-I /usr/include/slurm' make install
  - mkdir -p /var/lib/slurm/pyxis
  - echo "required /usr/local/lib/slurm/spank_pyxis.so runtime_path=/var/lib/slurm/pyxis" | tee /etc/slurm/plugstack.conf.d/pyxis.conf
  - mkdir -p /var/lib/slurm/enroot/ /var/spool/slurmd  /var/spool/enroot && chown -R slurm:slurm /var/lib/slurm/ /var/spool/slurmd/  /var/spool/enroot
  - mkdir -p /etc/systemd/system/slurmd.service.d/
  - chown slurm:slurm /etc/slurm/slurmdbd.conf
  - chown slurm:slurm /var/lib/slurm/slurmctld
  - echo 'export NCCL_TOPO_FILE=/etc/nccl-topo-h100-v1.xml' >> /home/slurm/.bashrc
  - sudo bash /tmp/setup_auth.sh

write_files:
- content: |
    ${indent(4, master_privkey)}
  path: /etc/ssh/rsa_id
  permissions: "0400"
  owner: root:root
- path: /etc/systemd/system/slurmd.service.d/local.conf
  content: |
    [Service]
    User=slurm
    Group=slurm
- path: /etc/enroot/enroot.conf.d/enroot.conf
  content: |
    ENROOT_RUNTIME_PATH        /var/spool/enroot/user-$(id -u)
    ENROOT_CONFIG_PATH         $${HOME}/enroot
    ENROOT_CACHE_PATH          /var/spool/enroot
    ENROOT_DATA_PATH           /var/spool/enroot/data/user-$(id -u)
    ENROOT_SQUASH_OPTIONS -noI -noD -noF -noX -no-duplicates
    ENROOT_ROOTFS_WRITABLE     yes
    ENROOT_MOUNT_HOME          no
    ENROOT_RESTRICT_DEV        no
- path: /etc/slurm/slurm.conf
  content: |
    # slurm.conf file generated by configurator.html.
    # Put this file on all nodes of your cluster.
    # See the slurm.conf man page for more information.
    #
    ClusterName=Slurm-cluster
    SlurmctldHost=node-master

    SlurmUser=slurm
    SlurmdUser=root
    SlurmctldPort=6817
    SlurmdPort=6818
    AuthType=auth/munge
    StateSaveLocation=/var/lib/slurm/slurmctld
    SwitchType=switch/none
    MpiDefault=pmi2
    SlurmctldPidFile=/run/slurmctld.pid
    SlurmdPidFile=/run/slurmd.pid
    ProctrackType=proctrack/pgid
    ReturnToService=0

    # TIMERS
    SlurmctldTimeout=300
    SlurmdTimeout=300
    InactiveLimit=0
    MinJobAge=300
    KillWait=30
    Waittime=0

    # DEBUG
    DebugFlags=NO_CONF_HASH

    # LOGGING/ACCOUNTNG
    SlurmctldDebug=info
    SlurmctldLogFile=/var/log/slurm/slurmctld.log
    SlurmdDebug=info
    SlurmdLogFile=/var/log/slurm/slurmd.log
    JobAcctGatherType=jobacct_gather/none

    #DB
    %{ if is_mysql }
    AccountingStorageType=accounting_storage/slurmdbd
    AccountingStorageHost=node-master
    JobCompType=jobcomp/mysql
    JobCompUser=slurm
    JobCompPass= ${password}
    JobCompHost= ${hostname}
    JobCompLoc=slurm-db
    %{ endif }

    GresTypes=gpu
    SelectType=select/cons_tres
    # COMPUTE NODES
    # NodeName=node-[1-${cluster_nodes_count}] CPUs=16 RealMemory=32090 State=idle State=UNKNOWN
    NodeName=node-[1-${cluster_nodes_count}] Gres=gpu:8 CPUs=160 RealMemory=1290080 State=idle State=UNKNOWN
    PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP
- path: /etc/slurm/cgroup.conf
  content: |
    CgroupReleaseAgentDir="/etc/slurm/cgroup"
- path: /etc/slurm/slurmdbd.conf
  permissions: '0600'
  content: |
    AuthType=auth/munge
    DbdHost=node-master
    DebugLevel=info
    LogFile=/var/log/slurm/slurmdbd.log
    PidFile=/run/slurmdbd.pid
    SlurmUser=slurm
    StoragePass=${password}
    StorageUser=slurm
    StorageHost=${hostname}
    StorageLoc=slurm-db
    StorageType=accounting_storage/mysql
- path: /tmp/setup_auth.sh
  content: |
    #!/bin/bash
    while true; do
        echo "Checking if munge is installed on master..." >> /var/log/slurm-startup.log
        # Execute the check command on the remote host and interpret the result directly in if
        if dpkg -l munge &>/dev/null; then
            echo "munge is installed on master" >> /var/log/slurm-startup.log
            break # Exit the loop if the program is found
        else
            echo "munge is not installed on master. Waiting..." >> /var/log/slurm-startup.log
        fi
        # Wait for a bit before checking again
        sleep 60 # Wait for 60 seconds
    done

    # Wait for the file to become available
    while [ ! -f "/etc/ssh/rsa_id" ]; do
        echo "Waiting for rsa_id to become available..." >> /var/log/slurm-startup.log
        sleep 20 # Wait for 20 second before checking again
    done

    N=${cluster_nodes_count}
    user='slurm'

    # Loop from 1 to N and perform operations on each node
    for ((i=1; i<=N; i++)); do
        node="node-$i"
        node_address="slurm@node-$i"

        echo "Loop until cloud-init status is not running on $node." >> /var/log/slurm-startup.log
        while :; do
            STATUS=$(ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'cloud-init status' 2>&1)
            echo "cloud-init is in $STATUS on $node." >> /var/log/slurm-startup.log
            if [[ $STATUS == *"status: running"* || $STATUS == *"status: not started"* || $STATUS == *"ssh: connect"* ]]; then
                echo "cloud-init is still running on $node. Waiting..." >> /var/log/slurm-startup.log
                sleep 15 # Wait for 5 seconds before checking again
            else
                echo "cloud-init is no longer running." >> /var/log/slurm-startup.log
                break # Exit the loop
            fi
        done

        echo "Proceeding with the rest of the script..." >> /var/log/slurm-startup.log
        echo "cloud-init check on $node" >> /var/log/slurm-startup.log
        ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'cloud-init status'
        echo "Copying MUNGE key to $node_address..." >> /var/log/slurm-startup.log
        ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'sudo rm -f /tmp/munge.key.tmp && sudo tee /etc/munge/munge.key > /dev/null' < /etc/munge/munge.key
        echo "Restarting MUNGE on $node_address..." >> /var/log/slurm-startup.log
        ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'sudo systemctl restart munge'
        sleep 5
        echo "Restarting SLURMD on $node_address..." >> /var/log/slurm-startup.log
        ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'sudo systemctl restart slurmd'
        echo "$node setup complete." >> /var/log/slurm-startup.log
    done
    systemctl restart slurmdbd
    sleep 5
    systemctl restart slurmctld
    echo "All nodes setup completed." >> /var/log/slurm-startup.log
    echo $PATH
- path: /etc/nccl-topo-h100-v1.xml
  content: |
    ${indent(4, file("files/nccl-topo-h100-v1.xml"))}
