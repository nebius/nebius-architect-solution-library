#cloud-config
users:
  - name: slurm
    groups: sudo
    shell: /bin/bash
    sudo: 'ALL=(ALL) NOPASSWD:ALL'
    ssh-authorized-keys:
      - ${sshkey}
      - ${master_pubkey}



packages:
  - munge
  - slurmdbd
  - slurm-client
  - slurmctld
  - ca-certificates
  - curl
  - gnupg
  - jq
  - numactl
  - make
  - gcc
  - libelf-dev
  - libssl-dev
  - bc
  - flex
  - bison
  - squashfs-tools
  - parallel
  - libnvidia-container-tools
  - pigz
  - squashfuse
  - fuse-overlayfs
  - bsdmainutils
  - ncal
  - libslurm-dev
  - nfs-common
  - glusterfs-client

runcmd:
 - systemctl disable slurmd; systemctl enable slurmctld; 
 - export DEBIAN_FRONTEND=noninteractive
 - wget https://github.com/NVIDIA/enroot/releases/download/v${ENROOT_VERSION}/enroot_${ENROOT_VERSION}-1_amd64.deb
 - wget https://github.com/NVIDIA/enroot/releases/download/v${ENROOT_VERSION}/enroot+caps_${ENROOT_VERSION}-1_amd64.deb
 - dpkg -i enroot_${ENROOT_VERSION}-1_amd64.deb
 - dpkg -i enroot+caps_${ENROOT_VERSION}-1_amd64.deb
 - git clone https://github.com/NVIDIA/pyxis.git && cd pyxis
 - CPPFLAGS='-I /usr/include/slurm' make && CPPFLAGS='-I /usr/include/slurm' make install
 - mkdir -p /var/lib/slurm/pyxis
 - echo "required /usr/local/lib/slurm/spank_pyxis.so runtime_path=/var/lib/slurm/pyxis" | tee /etc/slurm/plugstack.conf.d/pyxis.conf
 - mkdir -p /var/lib/slurm/enroot/ /var/spool/slurmd  /var/spool/enroot && chown -R slurm:slurm /var/lib/slurm/ /var/spool/slurmd/  /var/spool/enroot
 - mkdir -p /etc/systemd/system/slurmd.service.d/
 - chown slurm:slurm /etc/slurm/slurmdbd.conf
 - chown slurm:slurm /var/lib/slurm/slurmctld
 - echo 'export NCCL_TOPO_FILE=/etc/nccl-topo-h100-v1.xml' >> /home/slurm/.bashrc
 - sudo bash /tmp/setup_auth.sh

write_files:
- content: |
%{ for line in master_privkey ~}
    ${line}
%{ endfor ~}
  path: /etc/ssh/rsa_id
  permissions: "0400"
  owner: root:root
- path: /etc/systemd/system/slurmd.service.d/local.conf
  content: |
    [Service]
    User=slurm
    Group=slurm
- path: /etc/enroot/enroot.conf.d/enroot.conf
  content: |
    ENROOT_RUNTIME_PATH        /var/spool/enroot/user-$(id -u)
    ENROOT_CONFIG_PATH         $${HOME}/enroot
    ENROOT_CACHE_PATH          /var/spool/enroot
    ENROOT_DATA_PATH           /var/spool/enroot/data/user-$(id -u)
    ENROOT_SQUASH_OPTIONS -noI -noD -noF -noX -no-duplicates
    ENROOT_ROOTFS_WRITABLE     yes
    ENROOT_MOUNT_HOME          no
    ENROOT_RESTRICT_DEV        no
- path: /etc/slurm/slurm.conf
  content: |
    # slurm.conf file generated by configurator.html.
    # Put this file on all nodes of your cluster.
    # See the slurm.conf man page for more information.
    #
    ClusterName=Slurm-cluster
    SlurmctldHost=node-master

    SlurmUser=slurm
    SlurmdUser=root
    SlurmctldPort=6817
    SlurmdPort=6818
    AuthType=auth/munge
    StateSaveLocation=/var/lib/slurm/slurmctld
    SwitchType=switch/none
    MpiDefault=pmi2
    SlurmctldPidFile=/run/slurmctld.pid
    SlurmdPidFile=/run/slurmd.pid
    ProctrackType=proctrack/pgid
    ReturnToService=0

    # TIMERS
    SlurmctldTimeout=300
    SlurmdTimeout=300
    InactiveLimit=0
    MinJobAge=300
    KillWait=30
    Waittime=0


    #
    DebugFlags=NO_CONF_HASH

    # LOGGING/ACCOUNTNG
    SlurmctldDebug=info
    SlurmctldLogFile=/var/log/slurm/slurmctld.log
    SlurmdDebug=info
    SlurmdLogFile=/var/log/slurm/slurmd.log
    JobAcctGatherType=jobacct_gather/none

    #DB
    %{ if is_mysql }
    AccountingStorageType=accounting_storage/slurmdbd
    AccountingStorageHost=node-master
    JobCompType=jobcomp/mysql
    JobCompUser=slurm
    JobCompPass= ${password}
    JobCompHost= ${hostname}
    JobCompLoc=slurm-db
    %{ endif }


    GresTypes=gpu
    SelectType=select/cons_tres
    # COMPUTE NODES
    # NodeName=node-[1-${cluster_nodes_count}] CPUs=16 RealMemory=32090 State=idle State=UNKNOWN
    NodeName=node-[1-${cluster_nodes_count}] Gres=gpu:8 CPUs=160 RealMemory=1290080 State=idle State=UNKNOWN
    PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP
- path: /etc/slurm/cgroup.conf
  content: |
    CgroupReleaseAgentDir="/etc/slurm/cgroup"
- path: /etc/slurm/slurmdbd.conf
  permissions: '0600'
  content: |
    AuthType=auth/munge
    DbdHost=node-master
    DebugLevel=info
    LogFile=/var/log/slurm/slurmdbd.log
    PidFile=/run/slurmdbd.pid
    SlurmUser=slurm
    StoragePass=${password}
    StorageUser=slurm
    StorageHost=${hostname}
    StorageLoc=slurm-db
    StorageType=accounting_storage/mysql
- path: /tmp/setup_auth.sh
  content: |
    #!/bin/bash
    while true; do
        echo "Checking if munge is installed on master..."
        
        # Execute the check command on the remote host and interpret the result directly in if
        if dpkg -l munge &>/dev/null; then
            echo "munge is installed on master"
            break # Exit the loop if the program is found
        else
            echo "munge is not installed on master. Waiting..."
        fi
        
        # Wait for a bit before checking again
        sleep 60 # Wait for 60 seconds
    done

    # Wait for the file to become available
    while [ ! -f "/etc/ssh/rsa_id" ]; do
        echo "Waiting for rsa_id to become available..."
        sleep 20 # Wait for 1 second before checking again
    done

    N=${cluster_nodes_count}
    user='slurm'

    # Loop from 1 to N and perform operations on each node
    for ((i=1; i<=N; i++)); do
        node="node-$i"
        node_address="slurm@node-$i"

        # Loop until cloud-init status is not 'running'
        while :; do
            # Get the current status of cloud-init
            STATUS=$(ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'cloud-init status' 2>&1) # Redirect stderr to stdout to capture any errors

            if [[ $STATUS == *"status: running"* ]]; then
                echo "cloud-init is still running on $node. Waiting..."
                sleep 5 # Wait for 5 seconds before checking again
            else
                echo "cloud-init is no longer running."
                break # Exit the loop
            fi
        done
       
        echo "Proceeding with the rest of the script..."
        echo "cloud-init check on $node"
        ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'cloud-init status'


        echo "Copying MUNGE key to $node_address..."
        #sudo scp -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no /etc/munge/munge.key $node_address:/etc/munge/munge.key
        ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'sudo rm -f /tmp/munge.key.tmp && sudo tee /etc/munge/munge.key > /dev/null' < /etc/munge/munge.key
        
        echo "Restarting MUNGE on $node_address..."
        ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'sudo systemctl restart munge'
        sleep 2
        echo "Restarting SLURMD on $node_address..."
        ssh -i /etc/ssh/rsa_id -o StrictHostKeyChecking=no $node_address 'sudo systemctl restart slurmd'

        echo "$node setup complete."
    done
    systemctl restart slurmdbd
    sleep 5
    systemctl restart slurmctld

    echo "All nodes setup completed."
    echo $PATH
- path: /etc/nccl-topo-h100-v1.xml
  content: |
    <system version="1">
      <cpu numaid="0" affinity="00000000,00000000,0000ffff,ffffffff,ffffffff" arch="x86_64" vendor="GenuineIntel" familyid="6" modelid="106">
        <pci busid="0000:8a:00.0" class="0x060400" vendor="0x104c" device="0x8232" subsystem_vendor="0x0000" subsystem_device="0x0000" link_speed="32.0 GT/s PCIe" link_width="16">
          <pci busid="0000:8c:00.0" class="0x020700" vendor="0x15b3" device="0x101e" subsystem_vendor="0x15b3" subsystem_device="0x0023" link_speed="32.0 GT/s PCIe" link_width="16">
            <nic>
              <net name="mlx5_4" dev="4" speed="400000" port="1" latency="0.000000" maxconn="131072" gdr="1" coll="1"/>
            </nic>
          </pci>
          <pci busid="0000:8d:00.0" class="0x030200" vendor="0x10de" device="0x2330" subsystem_vendor="0x10de" subsystem_device="0x16c1" link_speed="32.0 GT/s PCIe" link_width="16"/>
        </pci>
        <pci busid="0000:8e:00.0" class="0x060400" vendor="0x104c" device="0x8232" subsystem_vendor="0x0000" subsystem_device="0x0000" link_speed="32.0 GT/s PCIe" link_width="16">
          <pci busid="0000:90:00.0" class="0x020700" vendor="0x15b3" device="0x101e" subsystem_vendor="0x15b3" subsystem_device="0x0023" link_speed="32.0 GT/s PCIe" link_width="16">
            <nic>
              <net name="mlx5_5" dev="5" speed="400000" port="1" latency="0.000000" maxconn="131072" gdr="1" coll="1"/>
            </nic>
          </pci>
          <pci busid="0000:91:00.0" class="0x030200" vendor="0x10de" device="0x2330" subsystem_vendor="0x10de" subsystem_device="0x16c1" link_speed="32.0 GT/s PCIe" link_width="16"/>
        </pci>
        <pci busid="0000:92:00.0" class="0x060400" vendor="0x104c" device="0x8232" subsystem_vendor="0x0000" subsystem_device="0x0000" link_speed="32.0 GT/s PCIe" link_width="16">
          <pci busid="0000:94:00.0" class="0x020700" vendor="0x15b3" device="0x101e" subsystem_vendor="0x15b3" subsystem_device="0x0023" link_speed="32.0 GT/s PCIe" link_width="16">
            <nic>
              <net name="mlx5_6" dev="6" speed="400000" port="1" latency="0.000000" maxconn="131072" gdr="1" coll="1"/>
            </nic>
          </pci>
          <pci busid="0000:95:00.0" class="0x030200" vendor="0x10de" device="0x2330" subsystem_vendor="0x10de" subsystem_device="0x16c1" link_speed="32.0 GT/s PCIe" link_width="16"/>
        </pci>
        <pci busid="0000:96:00.0" class="0x060400" vendor="0x104c" device="0x8232" subsystem_vendor="0x0000" subsystem_device="0x0000" link_speed="32.0 GT/s PCIe" link_width="16">
          <pci busid="0000:98:00.0" class="0x020700" vendor="0x15b3" device="0x101e" subsystem_vendor="0x15b3" subsystem_device="0x0023" link_speed="32.0 GT/s PCIe" link_width="16">
            <nic>
              <net name="mlx5_7" dev="7" speed="400000" port="1" latency="0.000000" maxconn="131072" gdr="1" coll="1"/>
            </nic>
          </pci>
          <pci busid="0000:99:00.0" class="0x030200" vendor="0x10de" device="0x2330" subsystem_vendor="0x10de" subsystem_device="0x16c1" link_speed="32.0 GT/s PCIe" link_width="16"/>
        </pci>
      </cpu>
      <cpu numaid="1" affinity="ffffffff,ffffffff,ffff0000,00000000,00000000" arch="x86_64" vendor="GenuineIntel" familyid="6" modelid="106">
        <pci busid="0000:a8:00.0" class="0x060400" vendor="0x104c" device="0x8232" subsystem_vendor="0x0000" subsystem_device="0x0000" link_speed="32.0 GT/s PCIe" link_width="16">
          <pci busid="0000:aa:00.0" class="0x020700" vendor="0x15b3" device="0x101e" subsystem_vendor="0x15b3" subsystem_device="0x0023" link_speed="32.0 GT/s PCIe" link_width="16">
            <nic>
              <net name="mlx5_0" dev="0" speed="400000" port="1" latency="0.000000" maxconn="131072" gdr="1" coll="1"/>
            </nic>
          </pci>
          <pci busid="0000:ab:00.0" class="0x030200" vendor="0x10de" device="0x2330" subsystem_vendor="0x10de" subsystem_device="0x16c1" link_speed="32.0 GT/s PCIe" link_width="16"/>
        </pci>
        <pci busid="0000:ac:00.0" class="0x060400" vendor="0x104c" device="0x8232" subsystem_vendor="0x0000" subsystem_device="0x0000" link_speed="32.0 GT/s PCIe" link_width="16">
          <pci busid="0000:ae:00.0" class="0x020700" vendor="0x15b3" device="0x101e" subsystem_vendor="0x15b3" subsystem_device="0x0023" link_speed="32.0 GT/s PCIe" link_width="16">
            <nic>
              <net name="mlx5_1" dev="1" speed="400000" port="1" latency="0.000000" maxconn="131072" gdr="1" coll="1"/>
            </nic>
          </pci>
          <pci busid="0000:af:00.0" class="0x030200" vendor="0x10de" device="0x2330" subsystem_vendor="0x10de" subsystem_device="0x16c1" link_speed="32.0 GT/s PCIe" link_width="16"/>
        </pci>
        <pci busid="0000:b0:00.0" class="0x060400" vendor="0x104c" device="0x8232" subsystem_vendor="0x0000" subsystem_device="0x0000" link_speed="32.0 GT/s PCIe" link_width="16">
          <pci busid="0000:b2:00.0" class="0x020700" vendor="0x15b3" device="0x101e" subsystem_vendor="0x15b3" subsystem_device="0x0023" link_speed="32.0 GT/s PCIe" link_width="16">
            <nic>
              <net name="mlx5_2" dev="2" speed="400000" port="1" latency="0.000000" maxconn="131072" gdr="1" coll="1"/>
            </nic>
          </pci>
          <pci busid="0000:b3:00.0" class="0x030200" vendor="0x10de" device="0x2330" subsystem_vendor="0x10de" subsystem_device="0x16c1" link_speed="32.0 GT/s PCIe" link_width="16"/>
        </pci>
        <pci busid="0000:b4:00.0" class="0x060400" vendor="0x104c" device="0x8232" subsystem_vendor="0x0000" subsystem_device="0x0000" link_speed="32.0 GT/s PCIe" link_width="16">
          <pci busid="0000:b6:00.0" class="0x020700" vendor="0x15b3" device="0x101e" subsystem_vendor="0x15b3" subsystem_device="0x0023" link_speed="32.0 GT/s PCIe" link_width="16">
            <nic>
              <net name="mlx5_3" dev="3" speed="400000" port="1" latency="0.000000" maxconn="131072" gdr="1" coll="1"/>
            </nic>
          </pci>
          <pci busid="0000:b7:00.0" class="0x030200" vendor="0x10de" device="0x2330" subsystem_vendor="0x10de" subsystem_device="0x16c1" link_speed="32.0 GT/s PCIe" link_width="16"/>
        </pci>
      </cpu>
    </system>
